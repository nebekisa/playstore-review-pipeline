# docker-compose.yml (Streamlined & Corrected)
version: '3.8'

services:
  # --- PostgreSQL for Airflow Metadata ---
  postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres_db_volume:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 5s
      retries: 5
    restart: always

  # --- Streamlit Dashboard ---
  streamlit-dashboard:
    # --- USE CUSTOM IMAGE OR INSTALL DEPS ON BUILD ---
    # Option 1: Use a custom Streamlit image (recommended for performance/stability)
    # build:
    #   context: .
    #   dockerfile: Dockerfile.streamlit # You need to create this Dockerfile
    # Option 2: Install dependencies at runtime (simpler for dev, slower startup)
    image: python:3.9-slim
    # --- END IMAGE CHOICE ---
    depends_on:
      postgres:
        condition: service_healthy
    volumes:
      # --- CRUCIAL: Mount the SQLite data volume ---
      # This makes the shared SQLite DB accessible inside the Streamlit container
      # at the path your Python code expects (/app/database/reviews.db)
      - sqlite_data:/app/database
      # --- Mount Dashboard Code ---
      - ./dashboard:/app/dashboard_code # Mount your local dashboard code directory
    working_dir: /app/dashboard_code # Set working directory inside container
    ports:
      - "8501:8501" # Map host port 8501 to container port 8501 (Streamlit default)
    # --- CORRECTED: Install deps and run Streamlit ---
    # Option 1 (if using custom image): command: streamlit run dashboard.py --server.port=8501 --server.address=0.0.0.0
    # Option 2 (if installing at runtime):
    
    command: >
      bash -c "
      pip install --no-cache-dir streamlit pandas plotly matplotlib seaborn wordcloud nltk
      streamlit run dashboard.py --server.port=8501 --server.address=0.0.0.0
      "
    # --- END CORRECTED ---
    restart: always # Restart if it crashes

  # --- Airflow Webserver ---
  airflow-webserver:
    # --- USE CUSTOM IMAGE ---
    build:
      context: . # Build context is the current directory
      dockerfile: Dockerfile.airflow # Use your custom Dockerfile that installs dependencies
    # --- END IMAGE ---
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      # --- Core Airflow Configuration ---
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__CORE__FERNET_KEY: ''
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      # --- Pass DB Path to Python scripts ---
      DB_PATH: /app/database/reviews.db
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      # --- Mount SQLite Data Volume ---
      - sqlite_data:/app/database
      # --- Mount Project Code (Scraper/Processor) ---
      - ./scraper:/opt/airflow/scraper_code
      - ./processor:/opt/airflow/processor_code
    user: "50000:50000"
    ports:
      - "8081:8080" # Map host port 8081 to container port 8080
    command: webserver
    restart: always

  # --- Airflow Scheduler ---
  airflow-scheduler:
    # --- USE CUSTOM IMAGE ---
    build:
      context: . # Build context is the current directory
      dockerfile: Dockerfile.airflow # Use your custom Dockerfile
    # --- END IMAGE ---
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      # --- Core Airflow Configuration ---
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__CORE__FERNET_KEY: ''
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      # --- Pass DB Path to Python scripts ---
      DB_PATH: /app/database/reviews.db
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - sqlite_data:/app/database # Share SQLite DB volume
      - ./scraper:/opt/airflow/scraper_code # Share Scraper code volume
      - ./processor:/opt/airflow/processor_code # Share Processor code volume
    user: "50000:50000"
    command: scheduler
    restart: always

  # --- Airflow Init (One-time setup) ---
  airflow-init:
    # --- USE CUSTOM IMAGE ---
    build:
      context: . # Build context is the current directory
      dockerfile: Dockerfile.airflow # Use your custom Dockerfile
    # --- END IMAGE ---
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      # --- Core Connection & Init Configuration ---
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__CORE__FERNET_KEY: '' # In production, generate a secure key
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false' # Don't load examples
      # --- Init Actions (Handled by Airflow's default entrypoint) ---
      _AIRFLOW_DB_MIGRATE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'
      _AIRFLOW_WWW_USER_USERNAME: admin
      _AIRFLOW_WWW_USER_PASSWORD: admin # CHANGE THIS FOR PRODUCTION!
      # --- Pass DB Path (if needed by init scripts, though unlikely) ---
      DB_PATH: /app/database/reviews.db
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - sqlite_data:/app/database # Share SQLite DB volume (if init needs it)
      - ./scraper:/opt/airflow/scraper_code
      - ./processor:/opt/airflow/processor_code
    user: "50000:50000"
    # --- Use Airflow's standard entrypoint which handles _AIRFLOW_* env vars ---
    # No need to override entrypoint/command manually anymore
    restart: "no" # Run only once

# --- Define Volumes (Must be top-level) ---
volumes:
  postgres_db_volume:
  sqlite_data: # Volume for SQLite database file
